---
title: Text Is All You Need
subtitle: Predicting Conflict Escalation Using Global News Content
author:
  - name: Diana Kolusheva
    email: dik431@g.harvard.edu
    affiliation: Harvard University
  - name: Chad Stoughton
    email: ch0793@g.harvard.edu
    affiliation: Harvard University
  - name: Evan Wheeler
    email: evw217@g.harvard.edu
    affiliation: Harvard University
lang: en
format:
  revealjs:
    incremental: true

---
## Introduction
- The task of forecasting conflict escalation is of great interest to policy makers, executives, and social scientists. 
- We introduce a conflict escalation definition and a novel binary classification metric.
- We outline 3 methods to extract generalizable conflict signals directly from a corpus of over 2.5 million document embeddings derived from global news sources using contemporary natural language processing and machine learning methods.

## Conflict forecasting

- Predicting conflict has long been of interest to researchers and practitioners of international relations, and statistical and computational methods to do so can be found in the literature as early as 1944. 
- Modern methodologies that resemble those we outline here began to appear in the literature in the early 2010s, when researchers began using holdout datasets for model validation. 
- This shift followed work by Ward et al. that sharply criticized the use of statistical significance tests on in-sample prediction to validate conflict models.

## Challenge of global models

- Today, much of the work in this area has continued to focus on country or region-specific models to predict conflict. 
- This introduces a “small-data problem” where relatively few observations of conflict are available for a given country. 
- Some efforts have been made to overcome this by building generalizable models that can theoretically be used to make predictions on any country.

:::{.notes}
- We followed the latter approach in developing our model, hypothesizing that the larger number of observations available to a global model will allow it to extract more generalizable signals of conflict, and that such a model will be better able to predict unforeseen risks in countries with little available training data.
:::

## Methodology

- Our approach is text-first, making predictions primarily or exclusively from text, with few or no other features added.
- To the best of our knowledge, no conflict prediction model described in the literature makes use of text embeddings.
- Previous efforts have included statistical analysis of text, and unsupervised Latent Dirichlet Allocation (LDA) for topic extraction.


## Defining Conflict Escalation

- Our task of modeling conflict escalation required a precise definition of what conflict escalation means. 
- We began with raw event and casualty counts from the [Armed Conflict Location & Event Data Project (ACLED)](https://acleddata.com/), whose team of researchers monitor thousands of sources in 20 languages to generate a dataset containing monthly totals of conflict events and their related fatalities . 

::: {.aside}
- Prior work has treated this as a binary classification task, asking whether or not conflict escalated in a given month, and we adopt this approach as well.
- In contrast to prior work, however, we do not treat the simple presence of a fatality due to conflict as sufficient to indicate conflict escalation. 
:::

::: {.notes}
- ACLED defines these events as incidents involving armed groups with political objectives as well as demonstrations, riots, and protests. With these totals in-hand, we needed to create a target to label our training data. 
- To achieve our objective of creating a generalizable conflict model, we required a metric that was consistent across varying local conditions. 
- Small changes in the absolute number of monthly fatalities can have widely different implications depending on the country and time period. 
:::

## Our 'Spike' conflict metric

- We derived a metric to highlight relative “spikes” in political violence.
- Two step calculation
  + First, we calculated the trend in violence for a given month
    - Defined by the slope of OLS regression of the number of ACLED-documented fatalities in target month & preceding 2 months. 
  + Next, we calculated whether that slope was in the 75th percentile of such slopes in the target country for the preceding 2 years.
 
:::{.notes}
This trend allowed us to estimate whether violence was increasing or decreasing in the target country at any time.
:::

## 'Spike' conflict metric example

![The Spike metric applied to Ukraine](imgs/Ukraine_Spikes.png){#fig-spike}


- Sensitive to small increases in political violence in peaceful countries/periods, while also filtering out small fluctuations in long-standing conflicts.

:::{.notes}

- @fig-spike shows how our metric captures conflict fluctuations in Ukraine. Prior to the Russian invasion of February 2022, small fluctuations trigger a spike. However, much larger fluctuations in the period following the invasion are filtered out, as the spike formula adjusts to the higher level of baseline conflict. 

- The increase in violence during the Ukrainian counter-offensive in the late summer again triggers a spike, and this reflects significant changes in the situation on the ground.
:::

## News Content Data

- The [Global Database of Events Language and Tone (GDELT) Project](https://www.gdeltproject.org/) provides public APIs of current events derived from news articles scraped from sources around the world . 
- In 2021, GDELT introduced its Global Similarity Graph Document Embeddings dataset, which precomputes 512-dimension document-level embeddings of their scraped news articles using the Universal Sentence Encoder.

:::{.notes}
- GDELT’s coverage over-represents the western world, particularly the United States and Europe.
- GDELT does represent Russia, India and China better than other event datasets, but it continues to underrepresent other parts of the world, particularly the global south.
- English is the most common language in the GDELT dataset, comprising more than 40% of the articles in its corpus.
:::

## Training Dataset

::: {#fig-dgp layout-ncol=1}
```{mermaid}
%%| fig-width: 5
%%| fig-height: 5
flowchart TD
    GSG[GDELT Global Similarity Graph] -->|sample 20% of daily files| Merge{Merge on article URL,\n select relevant features}
    GKG[GDELT Global Knowledge Graph] -->|sample 20% of daily files| Merge

    Merge --> GDELT(GDELT dataset)

    ACLED[ACLED monthly conflict totals] --> Spike{Calculate our lagged\n 'spike' binary variable}

    GDELT --> Data(Time-partitioned train, val, & test datasets)
    Spike --> Data
```
Data generation process
:::

:::{.notes}
- Our training data was derived from these GDELT and ACLED datasets. 
- GDELT news article embeddings served as our primary independent variable, while the “spike” metric derived from ACLED’s conflict data served as our target variable.
- The spike metric was used to label article data on a three-month time lag, allowing us to make predictions of future events.
- Our training dataset thus ran from January 2020 through October 2022. Data from November 2022 through March 2023 was used for model validation and testing.
:::

## Conflict Prediction Using Text and Country Features

- Our first approach employed a feed forward neural network model. 
- Uniform manifold approximation and projection (UMAP) was used to reduce dimensionality of GDELT article embeddings in a supervised manner.

---

![Scatterplot of 2-dimension UMAP-reduced articles with spike labels](imgs/fig-umap2d-ds-targets.png){#fig-umap2d-ds-targets}

:::{.notes}
- Our spike variable was used to condition the dimension reduction process of the training data to encourage separation of the variable classes in the reduced embedding space. 
- The fit UMAP model was applied to the validation and test data without supervision. These reduced, 128-dimension embeddings, along with country context indicators, were used as inputs to a feed forward neural network with a single hidden layer of 32 neurons. The model's 4577 parameters were trained for 3 epochs.
:::

## Additional country context inputs

- 2 numerical indicators from UNICEF's State of the World's Children report: 'Under-five mortality rate 2019' & 'Adolescent population 2020 Proportion of total population (%) Total'
- 3 one-hot-encoded categorical indicators: Human Development Index ranges, categories of gross national income per capita, & categories of Fragile States Index ranges)

:::{.notes}
- avoid the per-country 'small data problem' by allowing model to learn similarities between country contexts and generalize
:::

---

::: {#fig-ffnn layout-ncol=1}
```{mermaid}
%%| fig-width: 5
%%| fig-height: 5
flowchart TD
    GDELT[GDELT Embeddings] -->|sample by country by month| Articles(Monthly news article samples)

    Articles --> UMAP{Supervised UMAP}
    UMAP --> Compact(128 dimension representations)

    Compact --> FFNN{Feed Forward Neural Network}
    FFNN --> Pred(Prediction of conflict escalation in 3 months)

    ACLED[Monthly ACLED fatality counts] -->|compute our 'spike' variable| Spike
    Spike(Conflict response binary variable) --> |included during training|UMAP

    Spike -->FFNN

    Context[Country context indicators] --> FFNN
```
Feed forward neural network model architecture
:::

## Conflict Prediction Using Text and Country Labels

:::: {.columns}

::: {.column width="50%"}
::: {#fig-xgboost layout-ncol=1}
```{mermaid}
%%| fig-width: 5
%%| fig-height: 5
flowchart TD
    GDELT[GDELT Embeddings] --> |average by country by month| AGG(Monthly news representation)
    Countries[Country labels] --> |one-hot encoding| AGG

    AGG --> |PCA| PCA(169 dimension representation)
    PCA --> KNN{KNN}
    PCA --> XGD{XGBoost}

    XGD --> AVG{Mean}
    KNN --> AVG
    AVG --> Pred(Prediction of conflict escalation in 3 months)

    ACLED[Monthly ACLED fatality counts] --> |compute our 'spike' variable| Spike(Conflict response binary variable)
    Spike --> XGD
    Spike --> KNN
```
Statistical model architecture
:::
:::

::: {.column width="50%"}
- Average article embeddings per country per month
- One-hot encoded country labels
- Reduce dimensionality with PCA
- Train two models: K Nearest Neighbors and XGBoost
- Average their predictions
:::

::::

:::{.notes}
- In this approach we prepared the dataset by averaging news article embeddings per country, per month. We also included one-hot encoded columns for countries, but no other country features were added. 
- Given the high dimensionality of the data (512-dimensional embedding vectors + 700 dummy variables for countries), this dataset suffered from the “curse of dimensionality”. 
- This was handled by performing Principal Component Analysis; we found that reducing the number of dimensions to represent 80% of variance (~169 components with our selected train-test cutoff) improved the overall performance.
- Another issue was a class imbalance: conflict spike was present in only 14% of the training samples. We used random over-sampling technique to upsample the positive examples.
- The model used in this approach is a stacked model that averages probabilities independently predicted by a K-Nearest Neighbors model and an XGBoost model. We used standard open-source versions of the underlying models. The intention behind averaging the predicted probabilities was to reduce the impact of individual model weaknesses. 
:::

## Conflict Prediction Using Text Alone

- Our third approach used a pair of encoder-only transformers
- the “encoder” and the “classifier”.
- The encoder was used to distil the embedding corpus by analyzing a subset of article corpus and reducing it's dimensionality.
- The classifier could then be used to make final predictions on the resulting, distilled dataset.

---

:::: {.columns}

::: {.column width="50%"}
![Transformer Model](imgs/Transformer_Viz.png){#fig-transformer-viz}
:::

::: {.column width="50%"}
Training the Encoder

- Each country/month sample was split into subsamples of 50 articles each
- The same label was used for each subsample, creating multiple unique training samples per country per month 
- This produced a larger, but more distorted corpus
:::

:::: 

:::{.notes}

- By limiting the size of each sample to 50 articles, we reduced the dimensionality of each sample.
- By splitting the corpus into multiple unique training examples per country per month, we were able to substantially increase the size of the training dataset.
- This new dataset offered major advantages in combatting the curse of dimensionality, however, it also presented a distorted view of the world which over-represents countries that receive more media coverage and underrepresents those that receive less, amplifying the biases already present in the GDELT dataset. 
- Despite this limitation, the dataset was sufficient to train the encoder, which was fit to the modified dataset with the time-shifted spike metric as its target. 
:::

---

:::: {.columns}

::: {.column width="50%"}
![Transformer Model](imgs/Transformer_Viz.png){#fig-transformer-viz}
:::

::: {.column width="50%"}
Training the Classifier

- Once training was complete, the final layer of the encoder was removed, exposing the penultimate layer of 16 dense neurons. 
- Each vector was then stacked, resulting in a single training example for each country and month of size $16\times\frac{n}{50}$. 
:::

:::: 

:::{.notes}
- This produced a new, distilled, and undistorted dataset which could then be used to train the classifier using the same spike metric as its target. 
- In contrast to our earlier approaches, the dual-transformer model was trained purely on news article text embeddings. The model was not conditioned by country, and no additional features were added beyond the text corpus itself.
:::

---

::: {#fig-transformer layout-ncol=1}
```{mermaid}
%%| fig-width: 5
%%| fig-height: 5
flowchart TD
    GDELT[GDELT Embeddings] -->|sample by country by month| Articles(Monthly news article samples)

    Articles --> Trans1{Encoder\n transformer}
    Trans1 -->|classifier head present during training| Compact(16 dimension representations)

    Compact --> Trans2{Binary\n classifier\n transformer}
    Trans2 --> Pred(Prediction of conflict escalation in 3 months)

    ACLED[Monthly ACLED fatality counts] -->|compute our 'spike' variable| Spike
    Spike(Conflict response\n binary variable) --> Trans2

    Spike -->|included during model training| Trans1
```
Transformer model architecture
:::

## Results: metrics

![Model metrics (large values are better)](imgs/fig-best-max-metrics.png){#fig-best-max-metrics}

:::{.notes}
- The relative ranks of the models are the opposite for these two metrics with each of the two models with additional country data ranking best on one and worst on the other and the text-only model always being in the middle.
- We evaluated the models against a set of common performance metrics (see @fig-best-min-metrics, @fig-best-max-metrics, @fig-roc). 
- Despite conceptual differences in dataset creation and modeling techniques, all three of the described above approaches produced comparable results.
- Each of the models outperformed the other two in a subset of these metrics. 
- We assume that UNICEF decision makers would appreciate a tool that flags all potential rising conflicts even if sometimes it has false positives. At the same time, if the false positive rate is too high the signal would become too noisy for the tool to be useful. 
- Given these assumptions and the class imbalance of the original dataset, Recall (ratio of correctly identified conflict spikes to the total number of true spikes) and Cohen Kappa Score (overall agreement between predictions and true labels adjusted for class imbalance) seem to be most relevant metrics.
:::

---

## Results: comparison to prior work

:::: {.columns}

::: {.column width="50%"}
![Receiver Operator Characteristics](imgs/fig-roc.png){#fig-roc}
:::

::: {.column width="50%"}

- Our best ROC_AUC is 0.69
- Comparing to 0.83 overall and 0.75 for hard cases in paper by Mueller and Rauh 2022 (another binary classification problem)
- Comparison is not very meaningful because our target is different
:::
::::
:::{.notes}
- Our target was defined differently from related work which made direct comparison less meaningful.
- The closest approach used a presence or absence of any armed conflict related fatality as their binary target. They focused on ROC_AUC as their metric and were able to achieve 0.83 on overall dataset and 0.75 on hard cases (sudden conflict in peaceful countries) with their text based (LDA topic extraction) model. Our best ROC_AUC score is 69% (achieved by text + country labels XGBoost/KNN model).
:::

---

## Results: generalizability

![FFNN model accuracy on test data](imgs/fig-ffnn-test-accuracy.png){#fig-ffnn-test-accuracy height="100px"}


![XGBoost model accuracy on test data](imgs/fig-xgboost-test-accuracy.png){#fig-xgboost-test-accuracy height="100px"}


![Transformer model accuracy on test data](imgs/fig-transformer-test-accuracy.png){#fig-transformer-test-accuracy height="100px"}

:::{.notes}
- Further investigation showed that while overall numeric metrics look similar, different models appeared to perform better in different regions of the world—see @fig-ffnn-test-accuracy, @fig-xgboost-test-accuracy, and @fig-transformer-test-accuracy. 
:::

---

## Results: practicality

- Practical goal -- a tool that can assist UNICEF
- Cost & complexity are important
- Average monthly embeddings dataset & corresponding statistical model fit in memory
- Article-level embeddings in our other methods require additional data engineering steps to be processed in batches

:::{.notes}
- While performance is often the main objective when developing prediction models, our practical goal was a tool that can assist UNICEF, so cost & complexity is an important consideration.
- Average monthly embeddings dataset & corresponding statistical model fit in memory, whereas article-level embeddings in our other methods require additional data engineering steps to be processed in batches.
:::

## So which model is best?

- All three approaches produced promising results
- The model choice depends on:
  - user's goals
  - priority performance metrics
  - available resources


## Ethical Considerations
- Bias in the training data
- Adversarial attacks
- Self-fulfilling prophesies

:::{.notes}
- All models showed varying performance by region, could lead to resources being misallocated. 
- Large volumes of fake news could overwhelm true signal, obscure intentions of malicious actors & cause panic.
- AI predictions of impending conflict may lead policy makers to take actions that make conflict more likely. 
:::


## Mitigating Risk
- Models, not oracles 
- Not a substitute for careful deliberation and dialogue
- At their best, these models can provide an early warning for conflict, and if the right actions are taken, then they can be made to be wrong

:::{.notes}
- Measuring the temperature of the global media conversation, not gazing into the future.
:::

## Future work

- Increase size of training set (we only used 20%)
- Continue experimentation with data sampling and aggregation approaches and model tuning
- Research regional disparities in model performance
- Build an application with user interface
- Build a more robust and automated data pipeline

:::{.notes}
- Our work used only 20% of the daily embeddings available through GDELT: increasing size of training sets could yield better results. 
- Continued experimentation with data sampling and aggregation approaches, as well as model tuning, could produce models that outperform those outlined here without additional training data. 
- Finally, we believe deeper research into regional disparities in model performance is needed. Understanding the limitations of global media coverage for conflict prediction will be critical to any effort to operationalize this work.
:::

## Conclusions

- Text-only data vs data enriched with country labels and social indicators
- Conflict escalation definition
- Text embeddings provide a strong enough signal for predictions
- Comparable results across different data sampling and modeling approaches
- Neural network-based models -- better generalization across geographies
- Statistical model -- smaller size/cost of deployment

## So what?

- We hope this work will contribute to both UNICEF operations and political science research
- The performance results can serve as a benchmark for future research projects in this field

:::{.notes}
We explored the possibility of predicting future geopolitical conflict escalation using a global collection of news publications
:::
---

- The neural network-based models appear to generalize better across geographies while the statistical model provided a size/cost of deployment advantage.
- While this work did not produce a single best model for conflict escalation predictions, it demonstrated the possibility of such predictions from primarily text data. 
- We described several approaches for data aggregation and modeling. We hope this work will contribute to both UNICEF operations and political science research. 
- The performance results can serve as a benchmark for future research projects in this field.
