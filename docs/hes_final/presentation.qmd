---
title: Text Is All You Need
subtitle: Predicting Conflict Escalation Using Global News Content
author:
  - name: Diana Kolusheva
    email: dik431@g.harvard.edu
    affiliation: Harvard University
  - name: Chad Stoughton
    email: ch0793@g.harvard.edu
    affiliation: Harvard University
  - name: Evan Wheeler
    email: evw217@g.harvard.edu
    affiliation: Harvard University
lang: en
format:
  revealjs:
    incremental: true

---
## Introduction
- Forecasting conflict escalation is of interest to policy makers, executives, and social scientists. 
- In our paper, we introduce:
  + A novel binary classification metric for conflict escalation
  + Three methods to extract generalizable conflict signals directly from document embeddings 
    - Text with country characteristics  
    - Text with country labels
    - Text alone

:::{.notes}
- Embeddings derived from global news sources
:::

## Challenge of global models

- Most contemporary work focuses on country or region-specific models. 
  + This introduces a “small-data problem” 
- We chose to persue a global model, a relatively new area of research in this domain
  + This introduces a new problem of accounting for local conditions

:::{.notes}
- small data - some countries have not experienced much armed conflict, so very little training data
- local conditions - baseline monthly conflict fatalities vary widely from country to country, so importance of changes is relative
:::

## Defining Conflict Escalation

- Modeling conflict escalation requires a precise definition of what conflict escalation means. 
  + Many definitions in the literature.
- A global model requires a metric that is consistent acoss local conditions. 
  + The presence or absence of conflict is not sufficient. 

::: aside 
- We used data from from the [Armed Conflict Location & Event Data Project (ACLED)](https://acleddata.com/) for our raw event and fatality counts
:::

::: {.notes}
- prior work treats this as binary classification
- prior work uses the presence or absense of fatalities. This may work in local models
- for a global conflict model, we required a metric that was consistent across varying local conditions. 
:::

## Our "Spike" Conflict Metric
:::: {.columns}

::: {.column width="50%"}
![The Spike metric applied to Ukraine](imgs/Ukraine_Spikes.png){#fig-spike}

:::

::: {.column width="50%"}
- Highlights relative “spikes” in political violence.  
- Sensitive to small increases in political violence in peaceful countries/periods.
- Filters out small fluctuations in long-standing conflicts.
:::

:::: 

:::{.notes}
- Two step calculation
  + First, we calculated the trend in violence for a given month
    - Defined by the slope of OLS regression of the number of ACLED-documented fatalities in target month & preceding 2 months. 
  + Next, we calculated whether that slope was in the 75th percentile of such slopes in the target country for the preceding 2 years.
- @fig-spike shows how our metric captures conflict fluctuations in Ukraine. 
- Prior to the Russian invasion of February 2022, small fluctuations trigger a spike. 
- Larger absolute fluctuations of fatalities in following period are not considered 'spikes', as the spike formula adjusts to a higher conflict baseline. 
- The increase in fatalities during the Ukrainian counter-offensive in the late summer again triggers a spike, and this reflects significant escalation of the situation in the country.
:::

## Training Dataset

::: {#fig-dgp layout-ncol=1}
```{mermaid}
%%| fig-width: 5
%%| fig-height: 5
flowchart TD
    GSG[GDELT Global Similarity Graph] -->|sample 20% of daily files| Merge{Merge on article URL,\n select relevant features}
    GKG[GDELT Global Knowledge Graph] -->|sample 20% of daily files| Merge

    Merge --> GDELT(GDELT dataset)

    ACLED[ACLED monthly conflict totals] --> Spike{Calculate our lagged\n 'spike' binary variable}

    GDELT --> Data(Time-partitioned train, val, & test datasets)
    Spike --> Data
```
Data generation process
:::

:::{.notes}
- In contrast to prior work, our approach is text-first
  + Conflict signals are extracted primarily or exclusively from text
- Our training data was derived from 
- Global Database of Events Language and Tone (GDELT) Project
- Armed Conflict Location & Event Data Project
- GDELT news article embeddings served as our primary independent variable, while the “spike” metric derived from ACLED’s conflict data served as our target variable.
- The spike metric was used to label article data with a three-month time lag, allowing us to make predictions of future events.
- Previous efforts have included statistical analysis of text and topic extraction as supplementary features
- To the best of our knowledge, no conflict prediction model described in the literature makes use of text embeddings.
:::

## Conflict Prediction Using Text and Country Features

- Our first modeling approach employed a feed forward neural network model. 
- Uniform manifold approximation and projection (UMAP) was used to reduce dimensionality of GDELT article embeddings in a supervised manner.

---

![Scatterplot of 2-dimension UMAP-reduced articles with spike labels](imgs/fig-umap2d-ds-targets.png){#fig-umap2d-ds-targets}

:::{.notes}
- Reduction of 512 dimension embeddings to 128 dimension representation using UMAP
- dimensionality reduction process conditioned on our spike variable to encourage separation in the reduced embedding space
- The fit UMAP model was applied to the validation and test data without supervision. 
:::

---

::: {#fig-ffnn layout-ncol=1}
```{mermaid}
%%| fig-width: 5
%%| fig-height: 5
flowchart TD
    GDELT[GDELT Embeddings] -->|sample by country by month| Articles(Monthly news article samples)

    Articles --> UMAP{Supervised UMAP}
    UMAP --> Compact(128 dimension representations)

    Compact --> FFNN{Feed Forward Neural Network}
    FFNN --> Pred(Prediction of conflict escalation in 3 months)

    ACLED[Monthly ACLED fatality counts] -->|compute our 'spike' variable| Spike
    Spike(Conflict response binary variable) --> |included during training|UMAP

    Spike -->FFNN

    Context[Country context indicators] --> FFNN
```
Feed forward neural network model architecture
:::

:::{.notes}
- 128-dimension reduced embeddings, along with country context indicators, were used as inputs to a feed forward neural network with a single hidden layer of 32 neurons.
- intended to avoid the 'small data problem' in some countries by allowing the model to learn similarities between country contexts and generalize
- 2 numerical indicators from UNICEF's State of the World's Children report: 
  + 'Under-five mortality rate 2019' 
  + 'Adolescent population 2020 Proportion of total population (%) Total'
- 3 one-hot-encoded categorical indicators: 
  + Human Development Index ranges 
  + gross national income per capita
  + Fragile States Index ranges
:::

## Conflict Prediction Using Text and Country Labels

:::: {.columns}

::: {.column width="50%"}
::: {#fig-xgboost layout-ncol=1}
```{mermaid}
%%| fig-width: 5
%%| fig-height: 5
flowchart TD
    GDELT[GDELT Embeddings] --> |average by country by month| AGG(Monthly news representation)
    Countries[Country labels] --> |one-hot encoding| AGG

    AGG --> |PCA| PCA(169 dimension representation)
    PCA --> KNN{KNN}
    PCA --> XGD{XGBoost}

    XGD --> AVG{Mean}
    KNN --> AVG
    AVG --> Pred(Prediction of conflict escalation in 3 months)

    ACLED[Monthly ACLED fatality counts] --> |compute our 'spike' variable| Spike(Conflict response binary variable)
    Spike --> XGD
    Spike --> KNN
```
Statistical model architecture
:::
:::

::: {.column width="50%"}
- Average article embeddings per country per month
- One-hot encoded country labels
- Reduce dimensionality with PCA
- Train two models: K Nearest Neighbors and XGBoost
- Average their predictions
:::

::::

:::{.notes}
- In this approach we prepared the dataset by averaging news article embeddings per country, per month. We also included one-hot encoded columns for countries, but no other country features were added. 
- Given the high dimensionality of the data (512-dimensional embedding vectors + 700 dummy variables for countries), this dataset suffered from the “curse of dimensionality”. 
- This was handled by performing Principal Component Analysis; we found that reducing the number of dimensions to represent 80% of variance (~169 components with our selected train-test cutoff) improved the overall performance.
- Another issue was a class imbalance: conflict spike was present in only 14% of the training samples. We used random over-sampling technique to upsample the positive examples.
- The model used in this approach is a stacked model that averages probabilities independently predicted by a K-Nearest Neighbors model and an XGBoost model. We used standard open-source versions of the underlying models. The intention behind averaging the predicted probabilities was to reduce the impact of individual model weaknesses. 
:::

## Conflict Prediction Using Text Alone

- Our third approach used a pair of encoder-only transformers
- The “encoder” and the “classifier”.
- The encoder distils the embedding corpus by analyzing a subset of article corpus and reducing it's dimensionality.
- The classifier makes final predictions on the distilled dataset.
--- 

### The Encoder

:::: {.columns}

::: {.column width="50%"}
![Transformer Model](imgs/Transformer_Viz.png){#fig-transformer-viz}
:::

::: {.column width="50%"}

- Trained using a dataset that drew multiple samples per country per month of 50 articles each 
- This created multiple unique training examples for each country and month 
- Once trained, the Encoder's weights were frozen and penultimate layer was exposed
:::

:::: 

:::{.notes}

- By limiting the size of each sample to 50 articles, we reduced the dimensionality of each sample.
- By splitting the corpus into multiple unique training examples per country per month, we were able to substantially increase the size of the training dataset.
- This new dataset offered major advantages in combatting the curse of dimensionality, however, it also presented a distorted view of the world which over-represents countries that receive more media coverage and underrepresents those that receive less, amplifying the biases already present in the GDELT dataset. 
- Despite this limitation, the dataset was sufficient to train the encoder, which was fit to the modified dataset with the time-shifted spike metric as its target. 
:::

---

### The Classifier

:::: {.columns}

::: {.column width="50%"}
![Transformer Model](imgs/Transformer_Viz.png){#fig-transformer-viz}
:::

::: {.column width="50%"}
- Each country/month embedding matrix was passed into the Encoder in batches of 50 articles
- This produced a single distilled training example for each country and month
- This final, undistorted data set was then used for classification

:::

:::: 

:::{.notes}
- This produced a new, distilled, and undistorted dataset which could then be used to train the classifier using the same spike metric as its target. 
- In contrast to our earlier approaches, the dual-transformer model was trained purely on news article text embeddings. The model was not conditioned by country, and no additional features were added beyond the text corpus itself.
:::

## Ethical Considerations
- Bias in the training data
- Adversarial attacks
- Self-fulfilling prophesies

:::{.notes}
- GDELT’s coverage over-represents the western world, particularly the United States and Europe.
- GDELT does represent Russia, India and China better than other event datasets, but it continues to underrepresent other parts of the world, particularly the global south.
- English is the most common language in the GDELT dataset, comprising more than 40% of the articles in its corpus.
- All models showed varying performance by region, could lead to resources being misallocated. 
- Large volumes of fake news could overwhelm true signal, obscure intentions of malicious actors & cause panic.
- AI predictions of impending conflict may lead policy makers to take actions that make conflict more likely. 
:::


## Mitigating Risk
- Models, not oracles 
- Not a substitute for careful deliberation and dialogue
- At their best, these models can provide an early warning for conflict, and if the right actions are taken, then they can be made to be wrong

:::{.notes}
- Measuring the temperature of the global media conversation, not gazing into the future.
:::


## Results: metrics

![Model metrics (large values are better)](imgs/fig-best-max-metrics.png){#fig-best-max-metrics}

:::{.notes}
- The relative ranks of the models are the opposite for these two metrics with each of the two models with additional country data ranking best on one and worst on the other and the text-only model always being in the middle.
- We evaluated the models against a set of common performance metrics. 
- Despite conceptual differences in dataset creation and modeling techniques, all three of the described above approaches produced comparable results.
- Each of the models outperformed the other two in a subset of these metrics. 
- We assume that UNICEF decision makers would appreciate a tool that flags all potential rising conflicts even if sometimes it has false positives. At the same time, if the false positive rate is too high the signal would become too noisy for the tool to be useful. 
- Given these assumptions and the class imbalance of the original dataset, Recall (ratio of correctly identified conflict spikes to the total number of true spikes) and Cohen Kappa Score (overall agreement between predictions and true labels adjusted for class imbalance) seem to be most relevant metrics.
:::

---

## Results: comparison to prior work

:::: {.columns}

::: {.column width="50%"}
![Receiver Operator Characteristics](imgs/fig-roc.png){#fig-roc}
:::

::: {.column width="50%"}

- Comparison is not very meaningful because our target is different
- Comparing to 0.83 overall and 0.75 for hard cases in paper by Mueller and Rauh 2022 (another binary classification problem)
- Our best ROC_AUC is 0.69
:::
::::
:::{.notes}
- Our target was defined differently from related work which made direct comparison less meaningful.
- The closest approach used a presence or absence of any armed conflict related fatality as their binary target. They focused on ROC_AUC as their metric and were able to achieve 0.83 on overall dataset and 0.75 on hard cases (sudden conflict in peaceful countries) with their text based (LDA topic extraction) model. Our best ROC_AUC score is 69% (achieved by text + country labels XGBoost/KNN model).
:::

---

## Results: generalizability

::: {#fig-elephants layout-ncol=2}

![FFNN](imgs/fig-ffnn-test-accuracy.png){#fig-ffnn-test-accuracy}


![XGBoost](imgs/fig-xgboost-test-accuracy.png){#fig-xgboost-test-accuracy}


![Transformer](imgs/fig-transformer-test-accuracy.png){#fig-transformer-test-accuracy}

Model Accuracy on Test Set
:::


:::{.notes}
- Further investigation showed that while overall numeric metrics look similar, different models appeared to perform better in different regions of the world. 
:::

---

## Results: practicality

- Practical goal -- a tool that can assist UNICEF
- Cost & complexity are important
- Average monthly embeddings dataset & corresponding statistical model fit in memory
- Article-level embeddings in our other methods require additional data engineering steps to be processed in batches

:::{.notes}
- While performance is often the main objective when developing prediction models, our practical goal was a tool that can assist UNICEF, so cost & complexity is an important consideration.
- Average monthly embeddings dataset & corresponding statistical model fit in memory, whereas article-level embeddings in our other methods require additional data engineering steps to be processed in batches.
:::

## So which model is best?

- All three approaches produced promising results
- The model choice depends on:
  - user's goals
  - priority performance metrics
  - available resources


## Future work

- Increase size of training set (we only used 20%)
- Continue experimentation with data sampling and aggregation approaches and model tuning
- Research regional disparities in model performance
- Build an application with user interface
- Build a more robust and automated data pipeline

:::{.notes}
- Our work used only 20% of the daily embeddings available through GDELT: increasing size of training sets could yield better results. 
- Continued experimentation with data sampling and aggregation approaches, as well as model tuning, could produce models that outperform those outlined here without additional training data. 
- Finally, we believe deeper research into regional disparities in model performance is needed. Understanding the limitations of global media coverage for conflict prediction will be critical to any effort to operationalize this work.
:::

## Conclusions

- Text-only data vs data enriched with country labels and social indicators
- Conflict escalation definition
- Text embeddings provide a strong enough signal for predictions
- Comparable results across different data sampling and modeling approaches
- Neural network-based models -- better generalization across geographies
- Statistical model -- smaller size/cost of deployment

## So what?

- We hope this work will contribute to both UNICEF operations and political science research
- The performance results can serve as a benchmark for future research projects in this field

:::{.notes}
- We explored the possibility of predicting future geopolitical conflict escalation using a global collection of news publications
- The neural network-based models appear to generalize better across geographies while the statistical model provided a size/cost of deployment advantage.
- While this work did not produce a single best model for conflict escalation predictions, it demonstrated the possibility of such predictions from primarily text data. 
- We described several approaches for data aggregation and modeling. We hope this work will contribute to both UNICEF operations and political science research. 
- The performance results can serve as a benchmark for future research projects in this field.
:::

## Thank you

