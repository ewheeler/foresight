---
title: "Predicting Conflict Escalation"
author:
  - name: Diana Kolusheva
    email: email@h
    affiliation: Harvard University
  - name: Chad Stoughton
    email: email@h
    affiliation: Harvard University
  - name: Evan Wheeler
    email: email@h
    affiliation: Harvard University
lang: en
format:
  pdf:
    classoption:
    - twocolumn
    margin-left: 25mm
    margin-right: 25mm
    margin-top: 25mm
    margin-bottom: 25mm
    whitespace: medium
    mermaid:
      theme: neutral
bibliography: bibliography.bibtex
# https://github.com/quarto-dev/quarto-cli/discussions/2786
include-in-header:
  - text: |
      \usepackage{float}
      \makeatletter
      \let\oldlt\longtable
      \let\endoldlt\endlongtable
      \def\longtable{\@ifnextchar[\longtable@i \longtable@ii}
      \def\longtable@i[#1]{\begin{figure}[H]
      \onecolumn
      \begin{minipage}{0.5\textwidth}
      \oldlt[#1]
      }
      \def\longtable@ii{\begin{figure}[H]
      \onecolumn
      \begin{minipage}{0.5\textwidth}
      \oldlt
      }
      \def\endlongtable{\endoldlt
      \end{minipage}
      \twocolumn
      \end{figure}}
      \makeatother
# https://tex.stackexchange.com/questions/31301/how-to-reduce-the-number-of-hyphenation
include-before-body:
  - text: |
      \pretolerance=5000
      \tolerance=9000
      \emergencystretch=0pt
      \exhyphenpenalty=100
      \righthyphenmin=4
      \lefthyphenmin=5
      \setlength{\belowcaptionskip}{-5pt}
embed-resources: true
notebook-links: false
title-block-banner: true
abstract: |
  The task of forecasting conflict escalation is of great interest to policy makers, executives, and social scientists. In this article, we propose a novel, text-first approach to computational conflict prediction using contemporary natural language processing and machine learning techniques. We outline three methods to extract generalizable conflict signals directly from a corpus of ofer 2.5 million document embeddings derived from publicly available news sources. The first uses text data enriched by country context features, the second uses text data conditioned only on the country label itself, and the third extracts a signal from the text alone, with no additional country or region-specific features. We demonstrate that usable conflict signals can be extracted directly from text sources, and we outline potential avenues for further research that we believe may yield models that can provide early warnings to decision makers to facilitate strategic planning, resource allocation, and disaster mitigation. 

---
## Introduction

### Motivation
This project was undertaken in response to the acute needs of the UNICEF Emergency Operations Team. In 2021, UNICEF responded to 483 new and ongoing humanitarian situations in 153 countries (natural disasters, socio-political crises, health emergencies, nutrition crises, etc.) in addition to its global COVID-19 response efforts. This represents a sharp increase since 2018, and the number of disasters continues to climb. With the rising threat of the climate emergency, global shifts toward authoritarianism, and increasing levels of global conflict, innovative approaches are necessary to scale the reach of UNICEF’s impact to meet the increasing needs of vulnerable children. Automated means of identifying and escalating potentially negative situations as they unfold—or before they unfold—can help UNICEF’s emergency response functions be more proactive and efficient in responding quickly to people in need. 

### Objective

Our practical objective was to develop a tool to support UNICEF in conflict identification and response. Our research objective was to demonstrate that a generalizable, global conflict prediction model is viable, and that such a model can be built using primarily public text data. 

## Conflict forecasting

Predicting conflict has long been of interest to researchers and practitioners of international relations, and statistical and computational methods to do so can be found in the literature as early as 1944. Modern methodologies that resemble those we outline here began to appear in the literature in the early 2010s, when researchers began using holdout datasets for model validation [@10.2307/40730711]. This shift followed work by Ward et al that sharply criticized the use of statistical significance tests on in-sample prediction to validate conflict models [@doi:10.1177/0022343309356491]. Our methodology closely mirrors that which emerged in the following decade, including that described in more contemporary work, such as Mueller and Ruah [@RePEc:cam:camdae:2220]. In essence, this methodology involves using time-lagged input data to predict some conflict label that corresponds to a date some time after the input data would have become available. The model is then validated using an unseen holdout set, typically at the end of the date range available in the data [@doi:10.1080/03050629.2022.2070745]. This overcomes the problem of overfitting observed in earlier methods, and provides a principled starting point from which different models can be compared.

Today, much of the work in this area has continued to focus on country or region-specific models to predict conflict. This introduces a “small-data problem” where relatively few observations of conflict are available for a given country [@RePEc:cam:camdae:2220]. Some efforts have been made to overcome this by building generalizable models that can theoretically be used to make predictions on any country [@wen2023transformers]. We followed the latter approach in developing our model, hypothesizing that the larger number of observations available to a global model will allow it to extract more generalizable signals of conflict, and that such a model will be better able to predict unforeseen risks in countries with little available training data.

## Methodology

The principal difference between our approach and that of prior work is in our choice of predictive features, which we extract directly from text embeddings of publicly available news coverage. While newspaper and media content has been used in conflict prediction models before, it is generally only included as a supplement to models that primarily rely on other features. Despite this, models that include text features have been shown to outperform equivalent models that do not [@RePEc:cam:camdae:2220]. 

In contrast, our approach is text-first, making predictions primarily or exclusively from text, with few or no other features added. To the best of our knowledge, no conflict prediction model described in the literature makes use of text embeddings. Previous efforts have included statistical analysis of text, and unsupervised Latent Dirichlet Allocation (LDA) for topic extraction [@RePEc:cam:camdae:2220]. These approaches to Natural Language Processing (NLP) have relegated text data to a peripheral role in conflict prediction. 

In order to elevate text data to the center-stage of our prediction efforts, we drew from prior work in other fields. Our training data was structured such that each document is represented as an embedding vector, and multiple documents are combined with a label to create an aggregate picture of recent events. 

We developed three approaches for extracting conflict signals from text embeddings. The first approach used supervised dimensionality reduction and a feed-forward neural network conditioned with country context characteristics. In the second approach we used a fit-in-memory dataset of monthly embedding averages per country per month and applied statistical classification models such as K Nearest Neighbors and XGBoost. In the third approach we stacked multiple article-level embeddings and applied Transformer-based models to extract a signal directly from the text corpus. See the @fig-ffnn, @fig-xgboost, and @fig-transformer for details of modeling approaches.

### Defining Conflict Escalation
Our task of modeling conflict escalation required a precise definition of what conflict escalation is. We began with raw event and casualty counts from The [Armed Conflict Location & Event Data Project (ACLED)](https://acleddata.com/), whose team of researchers monitor thousands of sources in 20 languages to generate a dataset containing monthly totals of conflict events and their related fatalities [@doi:10.1177/0022343310378914]. ACLED defines these events as incidents involving armed groups with political objectives as well as demonstrations, riots, and protests. With these totals in-hand, we needed to create a target to label our training data. Prior work has treated this as a binary classification task, asking whether or not conflict escalated in a given month, and we adopt this approach as well. [@10.1093/jeea/jvac025]

In contrast to prior work, however, we do not treat the simple presence of a fatality due to conflict as sufficient to indicate conflict escalation. To achieve our objective of creating a generalizable conflict model, we required a metric that was consistent across varying local conditions. Small changes in the absolute number of monthly fatalities can have widely different implications depending on the country and time period. 

To address this, we derived a metric designed to highlight relative “spikes” in political violence. Calculating whether a spike occurred in a given month is a two-step process. First, we calculated the trend in violence for a given month, defined by the slope of an ordinary least squares regression of the number of ACLED-documented fatalities that occurred in the target month, and the preceding two months. This trend allowed us to estimate whether violence was increasing or decreasing in the target country at any given time. Next, we calculated whether that slope was in the $75^{th}$ percentile of such slopes in the target country for the preceding two years. This produced a binary metric that is sensitive to small increases in political violence in peaceful countries, while also filtering out small fluctuations in long-standing conflicts.

![The Spike metric applied to Ukraine](imgs/Ukraine_Spikes.png){#fig-spike}

@fig-spike shows how this metric captures conflict fluctuations in Ukraine. Prior to the Russian invasion of February 2022, small fluctuations were able to trigger a spike. However, much larger fluctuations in the period following the invasion are filtered out, as the spike formula adjusts to the higher level of baseline conflict. The increase in violence during the Ukrainian counter-offensive in the late summer again triggers a spike, and this reflects significant changes in the situation on the ground. 

### News Content Data

The [Global Database of Events Language and Tone (GDELT) Project](https://www.gdeltproject.org/) provides public APIs of current events derived from news articles scraped from sources around the world [@leetaru2013gdelt]. With updates every fifteen minutes, GDELT’s Event Database documents physical events such as diplomatic interactions, social unrest, and military action. Approximately 60 attributes are assigned to each event from GDELT’s processing and analysis, including named entities involved, geolocation details, categorization among 300 event types, as well as metrics related to average ‘tone’ of articles about the event and potential severity of event impact. In 2021, GDELT introduced its Global Similarity Graph Document Embeddings dataset, which precomputes 512 dimension news-article-level embeddings using the Universal Sentence Encoder [@gdeltgsg].

The GDELT Dataset has been used for a wide variety of research purposes, including for conflict prediction [@DBLP:conf/sac/ChenJY20]. While we believed this to be the best dataset available for our purposes, prior research has revealed some limitations that we must consider. GDELT’s coverage overrepresents the western world, particularly the United States and Europe. GDELT does represent Russia, India and China better than other event datasets, but it continues to underrepresent other parts of the world, particularly the global south. English is the most common language in the GDELT dataset, comprising more than 40% of the articles in its corpus [@Kwak_An_2021].

### Training Dataset

While many of GDELT’s databases reflect news articles from 1 January 1979 onward, GDELT’s news article embeddings exist only from 1 January 2020, limiting the time period for which training data can be drawn. Availability of ACLED’s monthly event and fatality totals varies by country, but event and fatality totals are available for all countries since at least 1 January 2020.

Our training data was derived from these GDELT and ACLED data. GDELT news article embeddings served as our independent variable, while the “spike” metric derived from ACLED’s conflict data served as our target variable. The spike metric was used to label article data on a three-month time lag, allowing us to make predictions of future events. Hard dates were set for train, test, and validation splits, and none of our models saw data published after October 2022 during training, with the remaining months used for testing and validation.

::: {#fig-dgp layout-ncol=1}
```{mermaid}
%%| fig-width: 3
%%| fig-height: 5
flowchart TD
    GSG[GDELT Global Similarty Graph] -->|sample 20% of daily files| Merge{Merge on article URL,\n select relevant features}
    GKG[GDELT Global Knowledge Graph] -->|sample 20% of daily files| Merge

    Merge --> GDELT(GDELT dataset)

    ACLED[ACLED monthly conflict totals] --> Spike{Calculate our lagged\n 'spike' binary variable}

    GDELT --> Data(Time-partitioned train, val, & test datasets)
    Spike --> Data
```
Overview of data generation process for our conflict dataset 
:::

### Conflict Prediction Using Text and Country Features

Our first approach employed a feed forward neural network model. Uniform manifold approximation and projection (UMAP) was used to reduce dimensionality of GDELT article embeddings in a supervised manner [@sainburg2021parametric]. Our spike variable was used to condition the dimension reduction process of the training data to encourage separation of the variable classes in the reduced embedding space. The fit UMAP model, was applied to the validation and test data without supervision. These reduced, 128 dimension embeddings, along with country context indicators, were used as inputs to a feed forward neural network with a single hidden layer of 32 neurons. The model's 4577 parameters were trained for 3 epochs.

![Density of embeddings resulting from supervised UMAP reduction to 2 dimensions](imgs/fig-umap2d-jointplot.png){#fig-umap2d-jointplot}

![Scatterplot of 2-dimension UMAP-reduced articles with spike labels](imgs/fig-umap2d-ds-targets.png){#fig-umap2d-ds-targets}

Country context inputs consisted of two numerical indicators ('Under-five mortality rate 2019' and 'Adolescent population 2020 Proportion of total population (%) Total' from UNICEF's State of the World's Children report) and three one-hot encoded categorical indicators (3 categories of Human Development Index ranges [@hdi], 4 categories of gross national income per capita [@wb], and 3 categories of Fragile States Index ranges[@fsi]).

::: {#fig-ffnn layout-ncol=1}
```{mermaid}
%%| fig-width: 3
%%| fig-height: 3
flowchart TD
    GDELT[GDELT Embeddings] -->|sample by country by month| Articles(Monthly news article samples)

    Articles --> UMAP{Supervised UMAP}
    UMAP --> Compact(128 dimension representations)

    Compact --> FFNN{Feed Forward Neural Network}
    FFNN --> Pred(Prediction of conflict escalation in 3 months)

    ACLED[Monthly ACLED fatality counts] -->|compute our 'spike' variable| Spike
    Spike(Conflict response binary variable) --> |included during training|UMAP

    Spike -->FFNN

    Context[Country context indicators] --> FFNN
```
Feed forward neural network model architecture
:::
### Conflict Prediction Using Text and Country Labels

In this approach we prepared the dataset by averaging the embeddings per country per month. We also included one-hot encoded columns for countries, but no other country features were added. Since we only had access to embeddings starting in January 2020, the total length of this dataset was just a little over 6,000 samples (training, validation, and test sets combined). Given the high dimensionality of the data (512-dimensional embedding vectors + 700 dummy variables for countries), this dataset suffered from “curse of dimensionality”. This was handled by performing Principal Component Analysis – we found that reducing the number of dimensions to represent 80% of variance (~169 components with our selected train-test cutoff) improved the overall performance. Another issue was a class imbalance – conflict spike was present in only 14% of the training samples. We used random over-sampling technique to upsample the positive examples.

The model used in this approach is a stacked model that averages probabilities independently predicted by a K Nearest Neighbors model and an XGBoost model. We used standard open-source versions of the underlying models. The intention behind averaging the predicted probabilities was to reduce the impact of individual model weaknesses. 

::: {#fig-xgboost layout-ncol=1}
```{mermaid}
%%| fig-width: 3
%%| fig-height: 3
flowchart TD
    GDELT[GDELT Embeddings] -->|average by country by month| AGG(Monthly news representation)
    Countries[Country labels] -->|one-hot encoding| AGG

    AGG -->|PCA| PCA(169 dimension representation)
    PCA --> KNN{KNN}
    PCA --> XGD{XGBoost}

    XGD --> AVG{Mean}
    KNN --> AVG
    AVG --> Pred(Prediction of conflict escalation in 3 months)

    ACLED[Monthly ACLED fatality counts] -->|compute our 'spike' variable| Spike
    Spike(Conflict response\n binary variable) --> XGD
    Spike --> KNN
```
Statistical model architecture
:::
### Conflict Prediction Using Text Alone

Our third approach used a pair of transformer models to extract information directly from the GDELT embedding corpus.[@vaswani2017attention] Architecturally, both models are encoder-only transformers, but for simplicity, we will refer to the first of these models as “the encoder” and the second as the “classifier”.

The purpose of the encoder was to reduce the dimensionality of the embedding corpus by analyzing a subset of article embeddings and returning a single vector of substantially reduced size. This is roughly analogous to the role of a convolutional layer in a CNN. The classifier could then be used to make final predictions on the resulting distilled dataset.

To prepare the training dataset, our full sample of the GDELT corpus was split into a single $512\times n$ matrix for each country and month where $n$ is the number of articles published in that month related to that country. Each of these matrices was then split into several random subsamples of 50 articles each. Samples for countries in which $n<50$ were padded to $512\times50$. This process had two effects on training. First, by limiting the size of each sample to 50 articles, we reduced the dimensionality of each sample. Simultaneously, by splitting the corpus into multiple unique training examples per country per month, we were able to substantially increase the size of the training dataset.

This new dataset offered major advantages in combatting the curse of dimensionality, however, it also represented a distorted view of the world, which overrepresents countries that receive more media coverage and underrepresents those that receive less, amplifying the biases already present in the GDELT dataset. Despite this limitation, this dataset was sufficient to train the encoder, which was fit to the modified dataset with the time-shifted spike metric as its target. Once training was complete, the final layer of the encoder was removed, exposing the penultimate layer of 16 dense neurons. The output of these 16 neurons would be used in the classification stage

![Reducing Embedding Dimensionality with Transformer Encoder](imgs/Transformer_Viz.png){#fig-transformer-viz}

With the encoder's training complete, we returned to the original base dataset. Each country-month matrix was again split into $512\times50$ matrices. Each of these samples could then be passed into the encoder, which distilled it into a $length-16$ vector. Each vector was then stacked, resulting in a single training example for each country and month of size $16\times\frac{n}{50}$. This produced a new, distilled, and undistorted dataset which could then be used to train the classifier using the same spike metric as its target. In contrast to our earlier approaches, the dual-transformer model was trained purely on text embeddings. The model was not conditioned by country, and no additional features were added beyond the text corpus itself.

::: {#fig-transformer layout-ncol=1}
```{mermaid}
%%| fig-width: 3
%%| fig-height: 3
flowchart TD
    GDELT[GDELT Embeddings] -->|sample by country by month| Articles(Monthly news article samples)

    Articles --> Trans1{Encoder\n transformer}
    Trans1 -->|classifier head present during training| Compact(16 dimension representations)

    Compact --> Trans2{Binary\n classifier\n transformer}
    Trans2 --> Pred(Prediction of conflict escalation in 3 months)

    ACLED[Monthly ACLED fatality counts] -->|compute our 'spike' variable| Spike
    Spike(Conflict response\n binary variable) --> Trans2

    Spike -->|included during model training| Trans1
```
Transformer model architecture
:::
## Results

We evaluated the models against a set of common performance metrics (see Figures 4-9). Despite conceptual differences in dataset creation and modeling techniques, all three of the described above approaches produced comparative results. Each of the models outperformed the other two in a subset of these metrics. This suggests that the choice of the model architecture for this task would depend on specific application goals. We assume that UNICEF decision makers would appreciate a tool that flags all potential rising conflicts even if sometimes it has false positives. At the same time, if the false positive rate is too high the signal would become too noisy for the tool to be useful. Given these assumptions and the class imbalance of the original dataset, Recall (ratio of correctly identified conflict spikes to the total number of true spikes) and Cohen Kappa Score (overall agreement between predictions and true labels adjusted for class imbalance) seem to be most relevant metrics. Two models using country features seem to perform
better on these two metrics.

In this work we defined the target in a way that was not used in prior literature, we cannot directly compare our models' performance to others. The most similar work used a presense or absense of any armed conflict related fatality as their binary target  [@10.1093/jeea/jvac025]. They focused on ROC_AUC as their metric and were able to achieve 0.83 on overall dataset and 0.75 on hard cases (sudden conflict in peaceful countries) with their text based (LDA topic retrieval) model. Our best ROC_AUC score is 69% but the comparison is not very meaningful due to difference in targets.

![Model performance metrics (higher values are better)](imgs/tbl-best-max-metrics.png){#fig-best-max-metrics}

![Model performance metrics (lower values are better)](imgs/tbl-best-min-metrics.png){#fig-best-min-metrics}

![Receiver Operator Characteristics](imgs/fig-roc.png){#fig-roc}

![Precision Recall Curve](imgs/fig-prc.png){#fig-prc}

Further investigation showed that while overall numeric metrics look similar and there seems to be a slight performance advantage in using country data in addition to text features, the Transformer-based text-only model generalized better across the globe. Since our goal was to build a global model capable of predicting conflict escalation in any part of the world, the generalizability is a significant reason to prefer Transformer architecture and resampled dataset. 

![FFNN model predictions on test data](imgs/fig-ffnn-test-predictions.png){#fig-ffnn-test-predictions}

![XGBoost model predictions on test data](imgs/fig-xgboost-test-predictions.png){#fig-xgboost-test-predictions}

![Transformer model Predictions on test data](imgs/fig-transformer-test-predictions.png){#fig-transformer-test-predictions}

While performance is often the main objective to optimize for when developing prediction models, our practical goal was to develop a model that can assist UNICEF in their decision making. With that in mind, cost of deployment is also an important factor for model evaluation. Average embeddings dataset and the corresponding statistical model fit in memory while working with article level embeddings in two other proposed methods requires additional data engineering steps to be processed in batches.

We think that all three approaches produced promising results and the model choice depends on the developers' goals, performance metrics and available resources.

## Conclusions

In this work we explored the possibility of predicting future geo-political conflict escalation at a country level using a global collection of news publications. We explored different ways of aggregating the data and applied both statistical machine learning and deep learning models. The results suggest that text embeddings provide a strong enough signal to make such predictions. Different data sampling and modeling approaches produce comparable results based on common metrics. Transformer-based model generalizes better across geographies while statistical model provides a size/cost of deployment advantage.

## Future work

We foresee several ways to extend this work, and there are exciting opportunities for both further research and operationalizations.

### Future research directions

One of the potential research extensions of this work is to continue experimenting with data sampling and aggregation approaches. We used either monthly averages to create $512\times1$ vectors or article level samples to create $512\times50$ matrices in our dataset preparation. While we received some promising results, it is possible that the first approach makes the data too coarse and the second too noisy. Another alternative could be generating daily averages and combining them into $512\times31$ (largest number of days in a month) matrices. This could potentially be more sensitive to the signal than the monthly averages and less noisy than article level samples.

The transformer models have a great deal of room for hyperparameter tuning. Further experimentation with the size of the encoder output, the number of articles per training sample, and implementing overlapping strides during distillation could all improve the downstream performance of the classifier. It may also be possible to use the output of the encoder as input to some other classification model architecture. 

Finally, due to time and resource constraints in this project we used 20% subset of daily news articles. Increasing that percentage could potentially lead to better results.

### Future practical directions

Building an application with a user interface could be a natural extension of this work. We envision a dashboard where a user can view a world map where countries are heatmap colored according to the estimated probability of escalating conflict. Countries with highest risk can be flagged or shown to a user in a separate view. In addition to the visual part of this application it is important to ensure that the model is deployed and can receive new data input and make inference.

Building a more robust and automated data processing pipeline is another important direction. This would involve designing either a batch or streaming system that can reliably collect data, preprocess and aggregate it into required format, train the model and make predictions. The system should handle computational resources and cost constraints.

## References

::: {#refs}
:::
