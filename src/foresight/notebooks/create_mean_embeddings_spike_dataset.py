# -*- coding: utf-8 -*-
"""create_mean_embeddings_spike_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l09GQDYoWStVCo5q40grljIZjzBD1WTo

This notebook creates train, validation and test datasets by mapping mean
mean embeddings files and Spike labels.
"""

!curl ipinfo.io

gcp_project = 'foresight-375620'
gcp_bucket = 'frsght'

import google.auth
from google.colab import auth

# authenticate with gcp
auth.authenticate_user()
credentials, project_id = google.auth.default()



!gcloud config set project $gcp_project
!echo "deb http://packages.cloud.google.com/apt gcsfuse-bionic main" > /etc/apt/sources.list.d/gcsfuse.list
!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
!apt -qq update
!apt -qq install gcsfuse=0.41.12

# create colab instance directory
!mkdir -p $gcp_bucket
# mount gcp bucket to colab instance directory.
# at /content/gcp_bucket
!gcsfuse  --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $gcp_bucket $gcp_bucket
!gcsfuse  --implicit-dirs  --stat-cache-ttl 12h --type-cache-ttl 12h --stat-cache-capacity 65536 --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $gcp_bucket $gcp_bucket

!pip install gcsfs
# !python -m pip install dask distributed --upgrade

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
import dask.dataframe as dd

codes = pd.read_html('https://www.worlddata.info/countrycodes.php')[0]

codes = dict(zip(codes['Country'], codes['Fips 10']))
codes.update({
     'Bailiwick of Guernsey' : 'GK'
    ,'Bailiwick of Jersey' :'GE'
    ,'Czech Republic':'EZ'
    ,'Democratic Republic of Congo': 'CG'
    ,'eSwatini' :'WZ'
    ,'Micronesia':'FM'
    ,'Monaco':'MN'
    ,'Pitcairn':'PC'
    ,'Republic of Congo':'CF'
    ,'Saint-Barthelemy':'TB'
    ,'Saint-Martin':'RN'
    ,'South Georgia and the South Sandwich Islands': 'SX'
    ,'South Sudan':'OD'
    ,'United States': 'US'
})

labels = pd.read_csv("/content/frsght/acled_labels/ACLED_Labels_032723.csv")
# we only have embeddings starting from 2020
labels = labels[labels['Year'] >= 2020]

# Add columns needed to map to embeddings
labels['target_window'] = labels['Date'].str[:-3]
labels['fips'] = labels['Country'].map(codes)

labels.head()

# Separately store spikes with matching column names
spike_only = labels[['target_window', 'fips', 'Spike']]
spike_only['lagged_spike'] = spike_only['Spike']
spike_only = spike_only.drop(columns=['Spike'])
spike_only = spike_only.drop_duplicates()

spike_only.head()

df = dd.read_csv("/content/frsght/monthly_emb_means_w_acled_csv_exp/*.csv")
df = df.compute()
len(df)
# 6696

df.head()

def get_lagged_date(row):
    year, month = row['window'].split('-')
    # To get 3 months lag we need to add 2
    lagged_month = int(month) + 2
    lagged_year = int(year)
    if lagged_month == 13:
        lagged_month = 1
        lagged_year = lagged_year + 1
    elif lagged_month == 14:
        lagged_month = 2
        lagged_year = lagged_year + 1
    if lagged_month < 10:
        lagged_month = f'0{lagged_month}'
    return f'{lagged_year}-{lagged_month}'

df['target_window'] = df.apply(get_lagged_date, axis=1)
emb_cols = [col for col in df.columns if col.startswith('docembed')]
columns_to_keep = ['fips', 'window', 'target_window'] + emb_cols
df = df[columns_to_keep]

df = df.merge(spike_only, on=['fips', 'target_window'])

df.head()

print(f'From {len(df)} rows {sum(df["lagged_spike"])} have a Spike')

train = df[df['target_window'] < '2023-01']
test = df[df['target_window'] >= '2023-01']
print(f'Got {len(train)} training and {len(test)} test samples')
# Got 6133 training and 478 test samples

train.to_csv("/content/frsght/datasets/spike_monthly_emb_means_train.csv")
test.to_csv("/content/frsght/datasets/spike_monthly_emb_means_test.csv")



