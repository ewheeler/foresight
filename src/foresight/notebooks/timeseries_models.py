# -*- coding: utf-8 -*-
"""timeseries_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KT4UZSqo4tWaeByHK7hNGkk6OJFRPfIy

This notebook uses `darts` python framework for timeseries models on mean embeddings.
"""

!curl ipinfo.io

gcp_project = 'foresight-375620'
gcp_bucket = 'frsght'

import google.auth
from google.colab import auth

# authenticate with gcp
auth.authenticate_user()
credentials, project_id = google.auth.default()



# !pip install darts
# !gcloud config set project $gcp_project
# !echo "deb http://packages.cloud.google.com/apt gcsfuse-bionic main" > /etc/apt/sources.list.d/gcsfuse.list
# !curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
# !apt -qq update
# !apt -qq install gcsfuse=0.41.12

# # create colab instance directory
# !mkdir -p $gcp_bucket
# # mount gcp bucket to colab instance directory.
# # at /content/gcp_bucket
# !gcsfuse  --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $gcp_bucket $gcp_bucket
# !gcsfuse  --implicit-dirs  --stat-cache-ttl 12h --type-cache-ttl 12h --stat-cache-capacity 65536 --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $gcp_bucket $gcp_bucket

# !pip install gcsfs
# !python -m pip install dask distributed --upgrade

# lag between embedding and prediction
MONTHS_SHIFT = 1

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
import dask.dataframe as dd

from darts import TimeSeries
from darts.dataprocessing.transformers import Scaler, StaticCovariatesTransformer
from darts.models import NBEATSModel, RandomForest, LightGBMModel, XGBModel
from darts.metrics import mae

df = dd.read_csv("/content/frsght/monthly_emb_means_w_acled_csv_exp/*.csv")
df = df.compute()
len(df)
# 6696

# do not keep countries with too few records
countries = [country for country in df['fips'].unique() 
             if len(df[df['fips'] == country]) > 30]
len(countries)

df = df[df['fips'].isin(countries)]
len(df)

emb_cols = [col for col in df.columns if col.startswith('docembed')]

df.head()

def make_day(row):
    return row['window'] + '-01'

df['window_day'] = df.apply(make_day, axis=1)

df['any_fatalities'] = df['Fatalities'] > 0

print(f'From {len(df)} rows {sum(df["any_fatalities"])} have any fatalities')
# From 6328 rows 2589 have any fatalities

df.head()

all_series = TimeSeries.from_group_dataframe(df, time_col='window_day',
        value_cols=['any_fatalities']+emb_cols,
        group_cols=['fips'], fillna_value=0, fill_missing_dates=True, freq='MS')

all_series = [series.shift(MONTHS_SHIFT) for series in all_series]

embeddings = [series[emb_cols] for series in all_series]
any_fatalities = [series['any_fatalities'] for series in all_series]
# scaler = Scaler()
scaler = StaticCovariatesTransformer()
embeddings_scaled = scaler.fit_transform(embeddings)
fatalities_scaled = scaler.fit_transform(any_fatalities)



emb_train_val_pairs = [emb.split_before(len(emb) - 6) for emb in embeddings_scaled]
fatalities_train_val_pairs = [fatal.split_before(len(fatal) - 6) for fatal in fatalities_scaled]

emb_train, emb_val = list(zip(*emb_train_val_pairs))
fatalities_train, fatalities_val = list(zip(*fatalities_train_val_pairs))

all_results = {}

def register_model_result(func):
    def wrapper(model, *args, **kwargs):
        res = func(model, *args, **kwargs)
        all_results[model] = res
        return res
    return wrapper

# future_cov means whether to use past_covariates or future_covariates;
# this depends on individual model

@register_model_result
def train_and_eval_model(model, future_cov=False, **fit_kwargs):
    cov_kwarg = 'future_covariates' if future_cov else 'past_covariates'
    fit_kwargs.update({cov_kwarg: embeddings_scaled})
    predictions = []
    model.fit(fatalities_train, **fit_kwargs)
    for i in range(len(fatalities_train)):
        pred_fatalities = model.predict(series=fatalities_train[i], n=6,
                                        **{cov_kwarg: embeddings_scaled[1]},
                                        verbose=False)
        pred_fatalities = scaler.inverse_transform(pred_fatalities)
        predictions.append(pred_fatalities)
    res = np.mean(mae(fatalities_val, predictions))
    print(f'{model} has average MAE={res}')
    return res

train_and_eval_model(
    NBEATSModel(input_chunk_length=3, output_chunk_length=3, random_state=42),
    epochs=3, verbose=True)



train_and_eval_model(
    RandomForest(lags=3, lags_past_covariates=3, output_chunk_length=3))

train_and_eval_model(
    LightGBMModel(lags=3, lags_past_covariates=3, output_chunk_length=3))

train_and_eval_model(
    XGBModel(lags=3, lags_past_covariates=3, output_chunk_length=3))

all_results
# {<darts.models.forecasting.nbeats.NBEATSModel at 0x7f715b2ab4f0>: 0.2739545361615834,
#  <darts.models.forecasting.random_forest.RandomForest at 0x7f715e67e3d0>: 0.2398050682261209,
#  <darts.models.forecasting.lgbm.LightGBMModel at 0x7f715e6aca00>: 0.29040398447284876,
#  <darts.models.forecasting.xgboost.XGBModel at 0x7f715f0045b0>: 0.2396147225853637}

